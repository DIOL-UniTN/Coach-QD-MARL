@article{Cully_2018, doi = {10.1109/tevc.2017.2704781},
    url = {https://doi.org/10.1109%2Ftevc.2017.2704781},
    year = {2018}, month = {apr}, pages = {245--259},
    title = {Quality and Diversity Optimization: A Unifying Modular Framework},
    number = {2},
    volume = {22}, 
    journal = {{IEEE} Transactions on Evolutionary Computation}, 
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})}, 
    author = {Antoine Cully and Yiannis Demiris}, 
} 

@inproceedings{Nilsson_2021, 
    doi = {10.1145/3449639.3459304}, 
    url = {https://doi.org/10.1145%2F3449639.3459304}, 
    year = {2021}, 
    month = {jun}, 
    title = {Policy gradient assisted {MAP}-Elites}, 
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, 
    publisher = {{ACM}}, author = {Olle Nilsson and Antoine Cully}, 
} 

@inproceedings{Cully_2021, 
doi = {10.1145/3449639.3459326}, 
url = {https://doi.org/10.1145%2F3449639.3459326}, 
year = {2021}, 
month = {jun}, 
title = {Multi-emitter {MAP}-elites}, 
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference}, 
publisher = {{ACM}}, author = {Antoine Cully}, 
} 

@inproceedings{Tarapore_2016, 
    doi = {10.1145/2908812.2908875}, 
    url = {https://doi.org/10.1145%2F2908812.2908875}, 
    year = {2016}, 
    month = {jul}, 
    title = {How do Different Encodings Influence the Performance of the {MAP}-Elites Algorithm?}, 
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016}, 
    publisher = {{ACM}}, 
    author = {Danesh Tarapore and Jeff Clune and Antoine Cully and Jean-Baptiste Mouret}, 
}

 @article{Pugh_2016, 
    doi = {10.3389/frobt.2016.00040}, 
    url = {https://doi.org/10.3389%2Ffrobt.2016.00040}, 
    year = {2016}, 
    month = {jul}, 
    title = {Quality Diversity: A New Frontier for Evolutionary Computation}, 
    volume = {3}, 
    journal = {Frontiers in Robotics and {AI}}, 
    publisher = {Frontiers Media {SA}}, 
    author = {Justin K. Pugh and Lisa B. Soros and Kenneth O. Stanley}, 
 } 

 @incollection{Wiley2014, 
    doi = {10.1002/9781118884614.ch2}, 
    url = {https://doi.org/10.1002%2F9781118884614.ch2}, 
    year = {2014}, month = {oct}, pages = {12--37}, 
    title = {Single-Agent Reinforcement Learning}, 
    booktitle = {Multi-Agent Machine Learning}, 
    publisher = {John Wiley {\&} Sons, Inc.}, 
 } 


 @article{Busoniu_2008, 
    doi = {10.1109/tsmcc.2007.913919}, 
    url = {https://doi.org/10.1109%2Ftsmcc.2007.913919}, 
    year = {2008}, 
    month = {mar}, 
    pages = {156--172}, 
    title = {A Comprehensive Survey of Multiagent Reinforcement Learning}, 
    number = {2}, 
    volume = {38}, 
    journal = {{IEEE} Transactions on Systems, Man, and Cybernetics, 
    Part C (Applications and Reviews)}, 
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})}, 
    author = {Lucian Busoniu and Robert Babuska and Bart De Schutter}, 
 } 

 @incollection{Tan_1993, 
    doi = {10.1016/b978-1-55860-307-3.50049-6}, 
    url = {https://doi.org/10.1016%2Fb978-1-55860-307-3.50049-6}, 
    year = {1993}, 
    pages = {330--337}, 
    title = {Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents}, 
    booktitle = {Machine Learning Proceedings 1993}, 
    publisher = {Elsevier}, 
    author = {Ming Tan}, 
} 

 @phdthesis{Lu, 
    doi = {10.22215/etd/2012-09679}, 
    url = {https://doi.org/10.22215%2Fetd%2F2012-09679}, 
    title = {Multi-agent reinforcement learning in games}, 
    publisher = {Carleton University}, 
    author = {Xiaosong Lu}, 
 } 

 @inproceedings{NIPS2017_3323fe11,
    author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf},
    volume = {30},
    year = {2017}
}


@Article{app11114948,
    AUTHOR = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Spanò, Sergio},
    TITLE = {Multi-Agent Reinforcement Learning: A Review of Challenges and Applications},
    JOURNAL = {Applied Sciences},
    VOLUME = {11},
    YEAR = {2021},
    NUMBER = {11},
    ARTICLE-NUMBER = {4948},
    URL = {https://www.mdpi.com/2076-3417/11/11/4948},
    ISSN = {2076-3417},
    ABSTRACT = {In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application fields, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.},
    DOI = {10.3390/app11114948}
}

@inproceedings{NEURIPS2021_7ed2d345,
    author = {Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario  and Hari, Ananth  and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and Williams, Niall  and Lokesh, Yashas  and Ravi , Praveen },
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {15032--15043},
    publisher = {Curran Associates, Inc.},
    title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
    url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7ed2d3454c5eea71148b11d0c25104ff-Paper.pdf},
    volume = {34},
    year = {2021}
}


@inproceedings{pmlr-v139-leibo21a,
  title = 	 {Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot},
  author =       {Leibo, Joel Z and Due{\~n}ez-Guzman, Edgar A and Vezhnevets, Alexander and Agapiou, John P and Sunehag, Peter and Koster, Raphael and Matyas, Jayd and Beattie, Charlie and Mordatch, Igor and Graepel, Thore},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6187--6199},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/leibo21a/leibo21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/leibo21a.html},
  abstract = 	 {Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent’s behavior constitutes (part of) another agent’s environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.}
}
@misc{mohanty2020flatlandrl,
      title={Flatland-RL : Multi-Agent Reinforcement Learning on Trains}, 
      author={Sharada Mohanty and Erik Nygren and Florian Laurent and Manuel Schneider and Christian Scheller and Nilabha Bhattacharya and Jeremy Watson and Adrian Egli and Christian Eichenberger and Christian Baumberger and Gereon Vienken and Irene Sturm and Guillaume Sartoretti and Giacomo Spigler},
      year={2020},
      eprint={2012.05893},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{NEURIPS2020_7967cc8e,
 author = {Christianos, Filippos and Sch\"{a}fer, Lukas and Albrecht, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {10707--10717},
 publisher = {Curran Associates, Inc.},
 title = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/7967cc8e3ab559e68cc944c44b1cf3e8-Paper.pdf},
 volume = {33},
 year = {2020}
}